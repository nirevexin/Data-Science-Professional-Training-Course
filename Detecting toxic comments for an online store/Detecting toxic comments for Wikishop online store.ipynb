{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project for the Wikishop online store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\exeve\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\exeve\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exeve\\AppData\\Local\\Temp\\ipykernel_29400\\2178701824.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\exeve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\exeve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\exeve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\exeve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords as nltk_stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#!pip install pymystem3\n",
    "#from pymystem3 import Mystem\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r'C:\\Users\\exeve\\Downloads\\toxic_comments.csv'\n",
    "path2 = '/datasets/toxic_comments.csv'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(path1):\n",
    "        data = pd.read_csv(path1)\n",
    "    elif os.path.exists(path2):\n",
    "        data = pd.read_csv(path2)\n",
    "except:\n",
    "    print('Something went wrong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 'Unnamed: 0' is an index duplicate. Let's delete it.\n",
    "data = data.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lowercase\n",
    "data['text'] = data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41499     civility \\n\\nplease refrain from comments such...\n",
       "44226                   your ip edits have been discovered.\n",
       "56653     ]\\n\\nhi bhaddani i totally totally aunderstand...\n",
       "5931      \" november 2014 (utc)\\n\\n and  i don't believe...\n",
       "21428     please refrain from adding nonsense to wikiped...\n",
       "16159     \"\\nabout the communist, not hard to believe th...\n",
       "129569    guido\\nhello blueboy i had a feeling this woul...\n",
       "662       hebrew name of lydia \\nappologies to til eulen...\n",
       "113609    the article was written by the person in quest...\n",
       "132331                    yes yes. why you deleted my work?\n",
       "90717     \"\\n\\nhelp\\n\\nhelp! a user deleted my in-progre...\n",
       "9911      myself. you may leave a message at the talk pa...\n",
       "25662     about the missing motivation section of the ge...\n",
       "127839    \"\\n\\n file:b0000dz6ke.03. ss500 sclzzzzzzz .jp...\n",
       "139102    \":there are some major differences between the...\n",
       "87029     \"\\ni am going to write a \"\"how to maintain thi...\n",
       "119303    youth \\n\\ndoes anyone else agree that the yout...\n",
       "118398    75.52.253.78   \\n\\n bow wow  \\n\\nbow wow i hav...\n",
       "131296           that is very and disrepectful rude of you.\n",
       "152998    unconventional dentistry: part i. introduction...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts contain a lot of characters, numbers, etc. It needs to be cleared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formatted data size: 0.0001068115234375 MB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "size = sys.getsizeof(corpus)/1024/1024\n",
    "f'Formatted data size: {size} MB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    \n",
    "    tag_dict = {\"J\": wn.ADJ,\n",
    "                \"N\": wn.NOUN,\n",
    "                \"V\": wn.VERB,\n",
    "                \"R\": wn.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wn.NOUN)\n",
    "\n",
    "def lemmatize(text):\n",
    "    m = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized = [m.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
    "    output = ' '.join(lemmatized)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text) \n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the functions for good functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text: explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n",
      "Cleaned and lemmatized text: explanation why the edits make under my username hardcore metallica fan be revert they weren t vandalism just closure on some gas after i vote at new york doll fac and please don t remove the template from the talk page since i m retire now\n"
     ]
    }
   ],
   "source": [
    "print(\"Source text:\", corpus[0])\n",
    "print(\"Cleaned and lemmatized text:\", lemmatize(clear_text(corpus[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7269e694a63146cf9aefd1d457612cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_lemm = pd.Series(corpus).progress_apply(lambda x: lemmatize(clear_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation why the edits make under my userna...\n",
       "1    d aww he match this background colour i m seem...\n",
       "2    hey man i m really not try to edit war it s ju...\n",
       "3    more i can t make any real suggestion on impro...\n",
       "4    you sir be my hero any chance you remember wha...\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(corpus_lemm, data['toxic'],\n",
    "                                                                            test_size = 0.5,\n",
    "                                                                            random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79646,)\n",
      "(79646,)\n",
      "(79646,)\n",
      "(79646,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texts vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(stop_words = 'english',\n",
    "                         ngram_range = (1,2),\n",
    "                         min_df=3,\n",
    "                         max_df=0.9,\n",
    "                         use_idf=1,\n",
    "                        smooth_idf=1,\n",
    "                         sublinear_tf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.fit(features_train)\n",
    "\n",
    "features_train_tfidf = vector.transform(features_train)\n",
    "features_test_tfidf = vector.transform(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79646, 131306)\n",
      "(79646, 131306)\n"
     ]
    }
   ],
   "source": [
    "print(features_train_tfidf.shape)\n",
    "print(features_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will create a function for hyperparameters tuning. \n",
    "- There is not so much data involved and the precision requested is only 0.75, therefore, taking into account the resources consumed by the BERT, ELMO, LSTM, etc. models, the LinearSVC or Logistic Regression models should more than comply, so they are the ones that we are going to use.\n",
    "- We will choose the best of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(model, params, features, target, cv):\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(model,\n",
    "                      param_grid=params,\n",
    "                      cv = cv,\n",
    "                      scoring = 'f1',\n",
    "                      n_jobs = -1,\n",
    "                      verbose = 10\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    grid_search.fit(features, target)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (grid_search.best_params_, grid_search.best_score_,\n",
    "            grid_search.cv_results_['mean_fit_time'][grid_search.best_index_],\n",
    "            grid_search.cv_results_['mean_score_time'][grid_search.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classification (LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best parameters: {'C': 1} \n",
      "Best score: 0.7594930849730966 \n",
      "Fitting time: 4.880761543909709 \n",
      "Scoring time: 0.05551640192667643\n"
     ]
    }
   ],
   "source": [
    "LSVC = LinearSVC(random_state = 12345, class_weight = 'balanced')\n",
    "\n",
    "LSVC_params ={'C': range(1,31,7)}\n",
    "\n",
    "LSVC_results = grid(LSVC, LSVC_params, features_train_tfidf, target_train, 3)\n",
    "\n",
    "\n",
    "LSVC_param = LSVC_results[0]\n",
    "LSVC_score = LSVC_results[1]\n",
    "LSVC_fit = LSVC_results[2]\n",
    "LSVC_pred = LSVC_results[3]\n",
    "\n",
    "print('Best parameters:', LSVC_param,\n",
    "'\\nBest score:', LSVC_score,\n",
    "'\\nFitting time:', LSVC_fit,\n",
    "'\\nScoring time:', LSVC_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'C': 10} \n",
      "Best score: 0.7657188790976778 \n",
      "Fitting time: 11.827710390090942 \n",
      "Scoring time: 0.04787158966064453\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state = 12345,\n",
    "                             class_weight = 'balanced')\n",
    "\n",
    "log_params = {'C' : range(10,20,1)}\n",
    "\n",
    "log_results = grid(log_reg,\n",
    "                   log_params,\n",
    "                   features_train_tfidf,\n",
    "                   target_train,\n",
    "                   cv = 3)\n",
    "\n",
    "log_param = log_results[0]\n",
    "log_score = log_results[1]\n",
    "log_fit = log_results[2]\n",
    "log_pred = log_results[3]\n",
    "\n",
    "print('Best parameters:', log_param,\n",
    "'\\nBest score:', log_score,\n",
    "'\\nFitting time:', log_fit,\n",
    "'\\nScoring time:', log_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the models is almost the same, but we will choose LinearSVC due to its faster calculation speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the best model (LinearSCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSVC = LinearSVC(random_state = 12345, class_weight = 'balanced', C = 1)\n",
    "LSVC.fit(features_train_tfidf, target_train)\n",
    "preds = LSVC.predict(features_test_tfidf)\n",
    "score = f1_score(target_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LinearSCV F1-Score: 0.7640793339862879\n"
     ]
    }
   ],
   "source": [
    "print('Testing LinearSCV F1-Score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test sample is also higher than 0.75. Thus, the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the project the following tasks were solved:\n",
    "- We have created a corpus of texts for analysis.\n",
    "- We cleaned the corpus of signs, stop words, etc., and also lemmatized the corpus texts.\n",
    "- We converted the corpus texts into vectors for training and prediction.\n",
    "- We divided the data into training and test samples with a share of 50%.\n",
    "- We analyzed LinearSVC and logistic regression models; both models had the same accuracy, but we chose the LinearSVC model because of its training speed.\n",
    "- During testing, the accuracy turned out to be higher than the established minimum (F1 score >= 0.75) so the model is suitable and adequate and therefore recommended."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 4563,
    "start_time": "2023-10-04T19:51:52.095Z"
   },
   {
    "duration": 3320,
    "start_time": "2023-10-04T19:51:56.669Z"
   },
   {
    "duration": 36,
    "start_time": "2023-10-04T19:51:59.991Z"
   },
   {
    "duration": 56,
    "start_time": "2023-10-04T19:52:00.028Z"
   },
   {
    "duration": 41,
    "start_time": "2023-10-04T19:52:00.085Z"
   },
   {
    "duration": 51,
    "start_time": "2023-10-04T19:52:00.128Z"
   },
   {
    "duration": 2218,
    "start_time": "2023-10-04T19:52:00.180Z"
   },
   {
    "duration": 104,
    "start_time": "2023-10-04T19:52:02.399Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-04T19:52:24.930Z"
   },
   {
    "duration": 51,
    "start_time": "2023-10-04T19:56:53.166Z"
   },
   {
    "duration": 3490,
    "start_time": "2023-10-04T19:57:24.776Z"
   },
   {
    "duration": 1066,
    "start_time": "2023-10-04T19:57:28.270Z"
   },
   {
    "duration": 44,
    "start_time": "2023-10-04T19:57:29.338Z"
   },
   {
    "duration": 20,
    "start_time": "2023-10-04T19:57:29.384Z"
   },
   {
    "duration": 28,
    "start_time": "2023-10-04T19:57:29.406Z"
   },
   {
    "duration": 37,
    "start_time": "2023-10-04T19:57:29.436Z"
   },
   {
    "duration": 27,
    "start_time": "2023-10-04T19:57:29.475Z"
   },
   {
    "duration": 54,
    "start_time": "2023-10-04T19:57:29.503Z"
   },
   {
    "duration": 3462,
    "start_time": "2023-10-04T19:58:26.301Z"
   },
   {
    "duration": 1056,
    "start_time": "2023-10-04T19:58:29.769Z"
   },
   {
    "duration": 43,
    "start_time": "2023-10-04T19:58:30.827Z"
   },
   {
    "duration": 103,
    "start_time": "2023-10-04T19:58:30.877Z"
   },
   {
    "duration": 107,
    "start_time": "2023-10-04T19:58:30.983Z"
   },
   {
    "duration": 45,
    "start_time": "2023-10-04T19:58:31.094Z"
   },
   {
    "duration": 2290,
    "start_time": "2023-10-04T19:58:31.142Z"
   },
   {
    "duration": 26,
    "start_time": "2023-10-04T19:58:33.434Z"
   },
   {
    "duration": 2476,
    "start_time": "2023-10-05T10:37:37.794Z"
   },
   {
    "duration": 2565,
    "start_time": "2023-10-05T10:37:55.009Z"
   },
   {
    "duration": 41,
    "start_time": "2023-10-05T10:37:59.929Z"
   },
   {
    "duration": 16,
    "start_time": "2023-10-05T10:38:00.486Z"
   },
   {
    "duration": 15,
    "start_time": "2023-10-05T10:38:03.608Z"
   },
   {
    "duration": 170,
    "start_time": "2023-10-05T10:38:04.102Z"
   },
   {
    "duration": 278,
    "start_time": "2023-10-05T10:38:14.482Z"
   },
   {
    "duration": 16,
    "start_time": "2023-10-05T10:38:19.031Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-05T10:39:31.612Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-05T10:39:35.412Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-05T10:39:45.842Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-05T10:39:47.498Z"
   },
   {
    "duration": 157,
    "start_time": "2023-10-05T10:39:49.547Z"
   },
   {
    "duration": 466,
    "start_time": "2023-10-05T10:40:14.370Z"
   },
   {
    "duration": 1528,
    "start_time": "2023-10-05T10:40:29.108Z"
   },
   {
    "duration": 856025,
    "start_time": "2023-10-05T10:43:46.343Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-05T10:58:02.371Z"
   },
   {
    "duration": 8489,
    "start_time": "2023-10-06T09:57:02.675Z"
   },
   {
    "duration": 2686,
    "start_time": "2023-10-06T09:57:11.166Z"
   },
   {
    "duration": 41,
    "start_time": "2023-10-06T09:57:13.854Z"
   },
   {
    "duration": 20,
    "start_time": "2023-10-06T09:57:13.898Z"
   },
   {
    "duration": 29,
    "start_time": "2023-10-06T09:57:13.922Z"
   },
   {
    "duration": 339,
    "start_time": "2023-10-06T09:57:13.953Z"
   },
   {
    "duration": 14,
    "start_time": "2023-10-06T09:57:14.295Z"
   },
   {
    "duration": 19,
    "start_time": "2023-10-06T09:57:14.311Z"
   },
   {
    "duration": 40,
    "start_time": "2023-10-06T09:57:14.333Z"
   },
   {
    "duration": 49,
    "start_time": "2023-10-06T09:57:14.376Z"
   },
   {
    "duration": 96,
    "start_time": "2023-10-06T09:57:14.427Z"
   },
   {
    "duration": 1684,
    "start_time": "2023-10-06T09:57:14.525Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
